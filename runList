Reshma Pandey 9009010090


hadoop-2.7.0/bin/hdfs dfs -rmr -skipTrash /home/asadani/rampup/output*
hadoop-2.7.0/bin/hdfs dfs -rm -skipTrash /home/asadani/rampup/input/*
hadoop-2.7.0/bin/hdfs dfs -copyFromLocal ../user-traffic.log /home/asadani/rampup/input

 
hadoop-2.7.0/bin/hadoop jar /home/IMPETUS/asadani/codebase/rampup/sandbox/data-flattener/target/data-flattener-0.0.1-SNAPSHOT.jar com.asadani.flattener.JSONDataFlattener /home/asadani/rampup/input/user-traffic.log /home/asadani/rampup/output/


mvn exec:java


http://stackoverflow.com/questions/29784532/pig-keeps-trying-to-connect-to-job-history-server-and-fails


<property>
<name>mapreduce.jobhistory.address</name>
<value>cm:10020</value>
<description>Host and port for Job History Server (default 0.0.0.0:10020)</description>
</property>

ps ax | grep -e JobHistory

/home/asadani/rampup/output/days

0.154.253.186,1444501382355,5208,POST,user/register,example,asdf

A = load '/home/asadani/rampup/output/days/10/21_927-r-00021' USING PigStorage(',') AS (clientIPAddress:bytearray, timestamp:bytearray, zipcode:bytearray, httpMethod:bytearray, httpURL:bytearray, userId:bytearray, userAuthToken:bytearray, eventDate:int, eventMonth:int, eventYear:int);

create 'HITS_BY_DAY', 'output'

===============WORKING================

FLAT_DATA = load '/home/asadani/rampup/output/days/*' USING PigStorage(',') AS (clientIPAddress:chararray, timestamp:chararray, zipcode:chararray, httpMethod:chararray, httpURL:chararray, userId:chararray, userAuthToken:chararray, userBrowser:chararray,  eventDate:chararray, eventMonth:chararray, eventYear:chararray);

FLAT_DATA_WITH_DATE = foreach FLAT_DATA generate clientIPAddress, timestamp, zipcode, httpMethod, httpURL, userId, userAuthToken, userBrowser, CONCAT(CONCAT(CONCAT((chararray)eventDate, '-'), (chararray)eventMonth, '-'), (chararray)eventYear) AS eventDateCon:chararray;

GROUPED_BY_DAY = GROUP FLAT_DATA_WITH_DATE BY eventDateCon;

COUNT_HITS_BY_DAY = FOREACH GROUPED_BY_DAY GENERATE group as myVar, COUNT(FLAT_DATA_WITH_DATE) as countForDay;

COUNT_HITS_BY_DAY_SUMMARY = FOREACH COUNT_HITS_BY_DAY GENERATE (chararray)myVar as day, (int)countForDay as count;

ILLUSTRATE COUNT_HITS_BY_DAY_SUMMARY;

STORE COUNT_HITS_BY_DAY_SUMMARY INTO 'hbase://HITS_BY_DAY' USING org.apache.pig.backend.hadoop.hbase.HBaseStorage ('output:count');


===============WORKING================

<dependency>
	<groupId>org.cloudera.htrace</groupId>
	<artifactId>htrace-core</artifactId>
	<version>2.05</version>
</dependency>

 export HADOOP_CLASSPATH=$CLASSPATH:/home/IMPETUS/asadani/codebase/rampup/sandbox/data-generator/target/lib/htrace-core-2.05.jar:$HADOOP_CLASSPATH

		

STORE COUNT_HITS_BY_DAY_SUMMARY INTO 'hbase://pig_table' USING org.apache.pig.backend.hadoop.hbase.HBaseStorage ('cf:count');


PIG_CLASSPATH=/usr/lib/hbase/hbase.jar:/usr/lib/zookeeper/zookeeper-3.4.5-cdh4.4.0.jar /usr/bin/pig /home/training/Load_HBase_Customers.pig







B = GROUP A BY userAuthToken;

C = FOREACH B {
SESSION_ACTIVITY = ORDER A BY timestamp;
GENERATE SESSION_ACTIVITY, COUNT(A);
};


hadoop-2.7.0/sbin/mr-jobhistory-daemon.sh start historyserver


======================================

Most visited pages by users

create 'MOST_VISITED_PAGES_BY_USERS', 'count'



FLAT_DATA = load '/home/asadani/rampup/output/days/*' USING PigStorage(',') AS (clientIPAddress:chararray, timestamp:chararray, zipcode:chararray, httpMethod:chararray, httpURL:chararray, userId:chararray, userAuthToken:chararray, eventDate:chararray, eventMonth:chararray, eventYear:chararray);

FLAT_DATA_FILTERED = FILTER FLAT_DATA BY (httpURL matches 'view/home' or httpURL matches 'view/product' or httpURL matches 'view/category');


FLAT_DATA_WITH_MONTH = foreach FLAT_DATA_FILTERED generate clientIPAddress, timestamp, zipcode, httpMethod, httpURL, userId, userAuthToken, CONCAT(CONCAT((chararray)eventMonth, '-'), (chararray)eventYear) AS eventMonthCon:chararray;

GROUPED_BY_MONTH_PAGE = GROUP FLAT_DATA_WITH_MONTH BY (eventMonthCon, httpURL);

COUNT_HITS_PER_PAGE_BY_MONTH = FOREACH GROUPED_BY_MONTH_PAGE GENERATE group as groupKey, COUNT(FLAT_DATA_WITH_MONTH) as countForMonth;

COUNT_HITS_PER_PAGE_BY_MONTH_SUMMARY = FOREACH COUNT_HITS_PER_PAGE_BY_MONTH GENERATE groupKey, (int)countForMonth as count;

STORE COUNT_HITS_PER_PAGE_BY_MONTH_SUMMARY INTO 'hbase://MOST_VISITED_PAGES_BY_USERS' USING org.apache.pig.backend.hadoop.hbase.HBaseStorage ('count:count');







sudo service mysql start

ln -s /usr/share/java/mysql-connector-java-5.1.28.jar /home/IMPETUS/asadani/Installs/apache-hive-1.2.1-bin/lib/mysql-connector-java-5.1.28.jar

/home/IMPETUS/asadani/Installs/apache-hive-1.2.1-bin/lib

SOURCE /home/IMPETUS/asadani/Installs/apache-hive-1.2.1-bin/scripts/metastore/upgrade/mysql/hive-schema-0.10.0.mysql.sql;

CREATE USER 'hive'@'localhost' IDENTIFIED BY 'hive';

REVOKE ALL PRIVILEGES, GRANT OPTION FROM 'hive'@'localhost';

GRANT SELECT,INSERT,UPDATE,DELETE,LOCK TABLES,EXECUTE ON metastore.* TO 'hive'@'localhost';




$ sudo chkconfig mysql on

REGISTER ... jar;

FLAT_DATA = load '/home/asadani/rampup/output/days/*' USING PigStorage(',') AS (clientIPAddress:chararray, timestamp:chararray, zipcode:chararray, httpMethod:chararray, httpURL:chararray, userId:chararray, userAuthToken:chararray, eventDate:chararray, eventMonth:chararray, eventYear:chararray);

GROUPED_BY_SESSION = GROUP FLAT_DATA BY userAuthToken;

SOMETHING = FOREACH GROUPED_BY_SESSION GENERATE com.asadani.ca.pig.udf.BagIterator(FLAT_DATA) AS bar;






{originatingIPAddress:'212.178.197.156', timestamp:'1448173259236', httpData:{httpMethod:'PUT', httpUrl:'user/register', headerParamAuthToken:'34asdwertf2weralorjvirw23kdfgnir9805d', headerParamUserId:'rob6@yetanothermail.com'}, userBrowser:'Chrome', zipcode:'20020'}

hadoop-2.7.0/bin/hdfs dfs -rmr -skipTrash /home/asadani/rampup/output*
hadoop-2.7.0/bin/hdfs dfs -rm -skipTrash /home/asadani/rampup/input/*
hadoop-2.7.0/bin/hdfs dfs -copyFromLocal ../user-traffic.log /home/asadani/rampup/input

==========================================================================================================================


